[metadata]
name = openeval
description = "OpenEval for rigourous evaluation of open-domain code generation"
long_description = file: README.md
long_description_content_type = text/markdown
url = https://github.com/bigcode-project/open-eval
license = Apache-2.0
license_files = LICENSE
platform = any
classifiers =
    Operating System :: OS Independent
    Programming Language :: Python :: 3
    License :: OSI Approved :: Apache Software License

[options]
packages = find:
python_requires = >=3.8
dependency_links =
install_requires =
    wget>=3.2
    tempdir>=0.7.1
    multipledispatch>=0.6.0
    appdirs>=1.4.4
    numpy>=1.19.5
    tqdm>=4.56.0
    termcolor>=2.0.0
    fire>=0.6.0
    openai>=1.11.1

[options.entry_points]
console_scripts =
    openeval.evaluate = openeval.evaluate:main
    openeval.inputgen = openeval.inputgen:main
    openeval.sanitize = openeval.sanitize:main
    openeval.syncheck = openeval.syncheck:main
    openeval.evalperf = openeval.evalperf:main
    openeval.perf.sas = openeval.perf.sas:main
    openeval.legacy_sanitize = openeval.legacy_sanitize:main
    openeval.perf.sampling = openeval.perf.sampling:main
    openeval.perf.select_pe_tasks = openeval.perf.select_pe_tasks:main
    openeval.perf.select_pe_inputs = openeval.perf.select_pe_inputs:main

[options.extras_require]
perf = Pympler>=1.0.1
       rich>=12.3.0
       tree_sitter_languages >= 1.10.2
       cirron @ git+https://github.com/openeval/Cirron@master
